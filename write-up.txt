Analysis of CWRU Student Course Evaluations
Benjamin Kaplan and Stephen Johnson

Every semester, students at Case are asked to evaluate their classes. This provides a huge wealth of information but it is fairly useless it its raw form. For our analysis, we are utilizing all course evaluations submitted for undergraduate courses at Case Western from the fall of 1997 through the spring of 2006.  Although the surveys are voluntary, subjecting them to selection bias, they have a high response rate. We used a custom script (written in Python) to parse the data.

We also had to clean the data. In our initial parsing, we found several missing pieces of data. In particular, the data for ENGR 145 and CHEM 111 (the introduction to chemistry classes for engineers) left values blank in the data file. Also, there were some typos. A few times, the course code had the letter "O" instead of the number 0. Finally, we had to filter out irrelevant data. The first thing we did on reading in the data was to filter out graduate courses, which have a course number greater than 400. In addition, we had to filter out classes that had no responses for a particular category, such as rating the TA in courses that didn't have a TA. 

We observed several things with this wealth of data (we had over 9,000 data points). First of all, we know, with a confidence level of 99.9%, that classes for the college of arts and sciences rate higher than classes for the school of engineering. Using the scale where 0= poor and 4 = Excellent with the other variables in between, the 1595 classes in the School of Engineering received an average rating of 2.56, with a standard deviation of .63398. The 6510 classes in the College of Arts and Sciences had an average rating of 2.92 with a standard deviation of 0.65572. Using a 2-sample Student's T distribution gave a T score of about -20, which corresponds to a probability of less than .000001. In addition, an analysis of course difficulty shows that students consider engineering classes to be harder than Arts and Sciences classes. The engineering classes had an average difficulty score of 2.28 ± 0.54851 while the arts and sciences classes had an average difficulty of 2.05123 ± 0.52131. We also ran a T-test based on the students' ratings of the course TA. In this case, the Arts and Sciences rated significantly higher than Engineering. The College of Arts and Sciences rated their TAs 2.8 while the School of Engineering TAs had an average rating of 2.33. The 2-sample T-Test yielded a T value of -14.95. It's interesting to note, however, that a much higher percentage of the engineering classes rated the TAs (1345 out of 1595) than the Arts and Sciences classes (3108 out of 6510).

We also examined the early data from the SAGES program. The 466 SAGES classes in the dataset had an average course ranking of 2.380 ± 0.804. The other classes 8968 classes had an average ranking of 2.855 ± 0.655. This yields a T score of -15.09, suggesting that there is an extremely high probability that Sages classes rank lower than non-SAGES classes. However, the difference in workload between SAGES classes (2.12244±0.4) and non-SAGES classes (2.088±0.531) was not significant. Based on this early data, students were not pleased with the SAGES program even though the courses themselves weren't more time consuming than their other classes.

In addition to comparing the different groups of courses, we also searched for trends across time. Our data covers 19 semesters, from the fall of 1997 to the fall of 2006. We plotted the average score in several categories, stratified again by school. Attached are graphs containing the data and regression lines for the College of Arts and Sciences, the School Engineering, and SAGES. The Weatherhead School of Management and the Mandel Center were excluded due to an insufficient number of data points. Overall, we noticed that there was very little change during the 10 year period observed. Even though some of the regression lines were statistically significant (> 95%), the slope of the line was minute. One trend we noticed is that the SAGES results are very scattered. The results for SAGES vary greatly from year to year. The SAGES results also had high standard errors (marked on the charts). This is probably because SAGES was still in its experimental stages at the time and the seminar format means that the students' experiences will depend on the professor and the subject of the seminar.