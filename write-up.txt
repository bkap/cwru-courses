Analysis of CWRU Student Course Evaluations
Benjamin Kaplan and Stephen Johnson

Every semester, students at Case are asked to evaluate their classes. This provides a huge wealth of information but it is fairly useless it its raw form. For our analysis, we are utilizing all course evaluations submitted for undergraduate courses at Case Western from the fall of 1997 through the spring of 2006.  Although the surveys are voluntary, subjecting them to selection bias, they have a high response rate. We used a custom script (written in Python) to parse the data.

We observed several things with this wealth of data (we had over 9,000 data points). First of all, we know, with a confidence level of 99.9%, that classes for the college of arts and sciences rate higher than classes for the school of engineering. Using the scale where 0= poor and 4 = Excellent with the other variables in between, the 1595 classes in the School of Engineering received an average rating of 2.56, with a standard deviation of .63701. The 6510 classes in the College of Arts and Sciences had an average rating of 2.92 with a standard deviation of 0.65572. Using a 2-sample Student's T distribution gave a T score of about -20, which corresponds to a probability of less than .000001. In addition, an analysis of course difficulty shows that students consider engineering classes to be harder than Arts and Sciences classes. The engineering classes had an average difficulty score of 2.28 ± 0.54851 while the arts and sciences classes had an average difficulty of 2.05123 ± 0.52131.

We also examined the early data from the SAGES program. The 466 SAGES classes in the dataset had an average course ranking of 2.380 ± 0.803. The other classes 8969 classes had an average ranking of 2.855 ± 0.655. This yields a T score of -15.07, suggesting that there is an extremely high probability that Sages classes rank lower than non-SAGES classes. However, the difference in workload between SAGES classes (2.12244±0.4) and non-SAGES classes (2.088±0.531) was not significant. Based on this early data, students were not pleased with the SAGES program even though the courses themselves weren't more time consuming than their other classes.

In addition to comparing the different groups of courses, we also searched for trends across time. Our data covers 19 semesters, from the fall of 1997 to the fall of 2006. We plotted the average score in several categories, stratified again by school. Attached are graphs containing the data and regression lines for the College of Arts and Sciences, the School Engineering, and SAGES. The Weatherhead School of Management and the Mandel Center were excluded due to an insufficient number of data points. Overall, we noticed that there was very little change during the 10 year period observed. Even though some of the regression lines were statistically significant (> 95%), the slope of the line was minute. One trend we noticed is that the SAGES results are very scattered. The results for SAGES vary greatly from year to year. The SAGES results also had high standard errors (marked on the charts). This is probably because SAGES was still in its experimental stages at the time and the seminar format means that the students' experiences will depend on the professor and the subject of the seminar.